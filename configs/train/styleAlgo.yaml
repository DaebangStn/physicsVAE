test: False
debug: False

algo:
  name: styleAlgo
  memo: walk_2_disc
  motion_file:
#    assets/motions/amp_humanoid_walk.npy
#    assets/motions/reallusion_sword_shield/RL_Avatar_Atk_Jump_Motion.npy
    assets/motions/reallusion_sword_shield/locomotion_and_drop_recovery.yaml

model:
  name: style

network:
  name: style
  separate: True

  space:
    continuous:
      mu_activation: None
      sigma_activation: None
      mu_init:
        name: default
      sigma_init:
        name: const_initializer
        val: -2.9
      fixed_sigma: True

  mlp:
    units: [1024, 512]
    activation: relu
    initializer:
      name: default

  disc:
    units: [1024, 512]
    activation: relu
    initializer:
      name: default


checkpoint:  # from the project root
#  runs/keypointMaxObsTask_styleAlgo_18-12-34-36_walk_2_disc/nn/last_keypointMaxObsTask_ep_2100_rew_7137.912.pth
checkpoint_disc:  # from the project root
#  runs/AMP_traj-4-Humanoid_22-15-29-08/nn/Humanoid_converted.pth


hparam:
#  max_frames: 100000

  normalize_input: True
  normalize_value: True
  normalize_advantage: True
  clip_value: False
  reward_shaper: {}
  save_frequency: 100

  logger:
    log_action: True
    filename: plot

  # learning hyperparameters
  horizon_length: 32
  gamma: 0.99
  tau: 0.95
  e_clip: 0.2
  grad_norm: 1.0

  minibatch_size: 8192
  mini_epochs: 6

  learning_rate: 2e-5
  lr_schedule: constant
  bound_loss_type: bound
  bounds_loss_coef: 10
  critic_coef: 5
  entropy_coef: 0.0

  reward:
    task_scale: 0.0
    disc_scale: 2.0

  style:
    replay_buf:
      size: 200000
      store_prob: 0.01
      num_demo_update: 128

    disc:
      # denominator for collected data within the rollout
      input_divisor: 16
      num_obs: 140
#      num_obs: 125
      obs_traj_len: 10
      loss_coef: 5
      logit_reg_scale: 0.01
      reg_scale: 0.0001
      grad_penalty_scale: 5
      log_hist: False

seed: 1
