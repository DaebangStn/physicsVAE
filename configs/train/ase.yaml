test: False
debug: False

algo:
  name: ase
  memo:
  motion_file:
#    assets/motions/amp_humanoid_run.npy
#    assets/motions/reallusion_sword_shield/RL_Avatar_Atk_Jump_Motion.npy
#    assets/motions/reallusion_sword_shield/locomotion_and_drop_recovery.yaml
    assets/motions/reallusion_sword_shield/locomotion2.yaml

model:
  name: skill

network:
  name: skill
  separate: True

  space:
    continuous:
      mu_activation: None
      sigma_activation: None
      mu_init:
        name: default
      sigma_init:
        name: const_initializer
        val: -2.9
      fixed_sigma: True

  mlp:
    units: [1024, 512]
    activation: relu
    initializer:
      name: default

  disc:
    units: [1024, 512]
    activation: relu
    initializer:
      name: default


checkpoint:  # from the project root
#  runs/keypointMaxObsTask_styleAlgo_01-14-58-59_fixed_norm/nn/keypointMaxObsTask.pth

#  runs/Humanoid_00010000_converted.pth

checkpoint_disc:  # from the project root
#  runs/AMP_traj-4-Humanoid_22-15-29-08/nn/Humanoid_converted.pth

hparam:
  multi_gpu: False
  ppo: True
  mixed_precision: False
  normalize_input: True
  normalize_value: True
  reward_shaper:
    scale_value: 1
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  learning_rate: 2e-5
  lr_schedule: constant
  score_to_win: 20000
  max_epochs: 10000
  save_best_after: 50
  save_frequency: 100
  save_intermediate: True
  print_stats: True
  grad_norm: 1.0
  entropy_coef: 0.0
  truncate_grads: False
  e_clip: 0.2
  horizon_length: 32
  minibatch_size: 8192
  mini_epochs: 6
  critic_coef: 5
  clip_value: False
  seq_len: 4
  bounds_loss_coef: 10
  amp_obs_demo_buffer_size: 200000
  amp_replay_buffer_size: 200000
  amp_replay_keep_prob: 0.01
  amp_batch_size: 128
  amp_minibatch_size: 512
  disc_coef: 5
  disc_logit_reg: 0.01
  disc_grad_penalty: 5
  disc_reward_scale: 2
  disc_weight_decay: 0.0001
  normalize_amp_input: True
  enable_eps_greedy: False

  task_reward_w: 0.0
  disc_reward_w: 1.0


seed: 1
