test: False
debug: False

algo:
  name: highLevelAlgo
  memo:
  motion_file:
#    assets/motions/dataset_humanoid.yaml
#    assets/motions/amp_humanoid_run.npy
    assets/motions/reallusion_sword_shield/locomotion_and_drop_recovery.yaml
#    assets/motions/reallusion_sword_shield/locomotion2.yaml

model:
  name: core

network:
  name: core
  separate: True

  # config for space is the same as low-level policy
  space:
    continuous:
      mu_activation: None
      sigma_activation: None
      mu_init:
        name: default
      sigma_init:
        name: const_initializer
        val: -2.9
      fixed_sigma: True
      learn_sigma: False

  mlp:
    units: [1024, 512]
    activation: relu
    initializer:
      name: default


checkpoint:  # from the project root
#  runs/ep_3100_rew_6916.521_3100.pth


hparam:
  normalize_input: True
  normalize_value: True
  normalize_advantage: True
  clip_value: False
  reward_shaper: {}
  save_frequency: 1000

  # learning hyperparameters
  horizon_length: 32
  gamma: 0.99
  tau: 0.95
  e_clip: 0.2
  grad_norm: 1.0

  minibatch_size: 4000
  mini_epochs: 6

  learning_rate: 2e-5
  lr_schedule: constant

  bound_loss_type: bound
  bounds_loss_coef: 10
  critic_coef: 5
  entropy_coef: 0.0

  reward:
    task_weight: 1.0
    task_scale: 0.5
    disc_scale: 2.0
    show_on_player: False

  llc:
    steps: 5
    cfg: archived_ckpt/ASE_Humanoid_11-16-57-18_loco4/algo_config_from_other.yaml
    ckpt: archived_ckpt/ASE_Humanoid_11-16-57-18_loco4/Humanoid_ep_18500_rew_1155.2125_18500_converted.pth


seed: 1
